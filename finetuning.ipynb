{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 10:25:58.198445: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-06 10:25:58.424745: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-06 10:25:58.424810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-06 10:25:58.446471: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 10:25:58.496414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 10:25:59.630746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained models and scheduler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch CompVis/ldm-celebahq-256: CompVis/ldm-celebahq-256 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "/home/cqilab/anaconda3/envs/sgrs/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "An error occurred while trying to fetch CompVis/ldm-celebahq-256: CompVis/ldm-celebahq-256 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "/home/cqilab/anaconda3/envs/sgrs/lib/python3.11/site-packages/diffusers/configuration_utils.py:245: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "The config attributes {'timestep_values': None, 'timesteps': 1000} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of U-Net: 41.79% (114457504/273915936)\n",
      "Sparsity of VQ-VAE: 48.07% (26575752/55287954)\n",
      "Initializing the LDMPipeline with pruned models...\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/938 [00:00<?, ?it/s]/home/cqilab/anaconda3/envs/sgrs/lib/python3.11/site-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDIMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDIMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "Training:  17%|█▋        | 162/938 [04:16<20:31,  1.59s/it, loss=0.0192]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from diffusers import LDMPipeline, UNet2DModel, VQModel, DDIMScheduler\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display\n",
    "\n",
    "# ============================\n",
    "# 1. Configuration Parameters\n",
    "# ============================\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"dataset/celeba_hq_256\"\n",
    "PRUNED_UNET_PATH = \"pruned_unet_celebahq.pth\"\n",
    "PRUNED_VQVAE_PATH = \"pruned_vqvae_celebahq.pth\"\n",
    "FINE_TUNED_UNET_PATH_TEMPLATE = \"run/finetuned70/fine_tuned_pruned_unet_epoch_{}.pth\"\n",
    "FINE_TUNED_VQVAE_PATH_TEMPLATE = \"run/finetuned70/fine_tuned_pruned_vqvae_epoch_{}.pth\"\n",
    "OUTPUT_DIR = \"generated_images\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Pruning\n",
    "PRUNE_AMOUNT_UNET = 0.5  # 30% pruning\n",
    "PRUNE_AMOUNT_VQVAE = 0.5  # 30% pruning\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 2\n",
    "NUM_WORKERS = 4\n",
    "IMAGE_SIZE = 256  # Assuming CelebA-HQ images are 256x256\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================\n",
    "# 2. Define the Custom Dataset\n",
    "# ============================\n",
    "\n",
    "class CelebaHQDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset directory.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = [\n",
    "            os.path.join(root_dir, img) for img in os.listdir(root_dir)\n",
    "            if img.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image in case of error\n",
    "            image = Image.new(\"RGB\", (IMAGE_SIZE, IMAGE_SIZE), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# ============================\n",
    "# 3. Prepare DataLoader\n",
    "# ============================\n",
    "\n",
    "# Define transformations (resize and normalize as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset = CelebaHQDataset(root_dir=DATASET_PATH, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ============================\n",
    "# 4. Load and Prune the Models\n",
    "# ============================\n",
    "\n",
    "# Load the pretrained models and scheduler\n",
    "print(\"Loading pre-trained models and scheduler...\")\n",
    "unet = UNet2DModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"unet\")\n",
    "vqvae = VQModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"vqvae\")\n",
    "scheduler = DDIMScheduler.from_config(\"CompVis/ldm-celebahq-256\", subfolder=\"scheduler\")\n",
    "\n",
    "# Pruning function for the U-Net and VQModel\n",
    "def prune_model(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            # print(f\"Pruned layer: {name} - amount: {amount}\")\n",
    "    return model\n",
    "\n",
    "# Apply pruning\n",
    "unet = prune_model(unet, amount=PRUNE_AMOUNT_UNET)\n",
    "vqvae = prune_model(vqvae, amount=PRUNE_AMOUNT_VQVAE)\n",
    "\n",
    "# Permanently remove pruning reparameterization to make pruning effective\n",
    "def remove_pruning(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.remove(module, 'weight')\n",
    "            # print(f\"Removed pruning reparameterization for {module}\")\n",
    "\n",
    "remove_pruning(unet)\n",
    "remove_pruning(vqvae)\n",
    "\n",
    "\n",
    "# unet.load_state_dict(torch.load(unet_path, map_location=DEVICE))\n",
    "# vqvae.load_state_dict(torch.load(vqvae_path, map_location=DEVICE))\n",
    "# # Save the pruned models\n",
    "# torch.save(unet.state_dict(), PRUNED_UNET_PATH)\n",
    "# torch.save(vqvae.state_dict(), PRUNED_VQVAE_PATH)\n",
    "# print(f\"Pruned models saved as {PRUNED_UNET_PATH} and {PRUNED_VQVAE_PATH}\")\n",
    "\n",
    "# Calculate and print sparsity\n",
    "def calculate_sparsity(model, model_name=\"Model\"):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += (param == 0).sum().item()\n",
    "    sparsity = 100.0 * zero_params / total_params\n",
    "    print(f\"Sparsity of {model_name}: {sparsity:.2f}% ({zero_params}/{total_params})\")\n",
    "\n",
    "calculate_sparsity(unet, \"U-Net\")\n",
    "calculate_sparsity(vqvae, \"VQ-VAE\")\n",
    "\n",
    "# ============================\n",
    "# 5. Initialize the Pipeline\n",
    "# ============================\n",
    "\n",
    "# Initialize the pipeline with the pruned U-Net and VQ-VAE\n",
    "print(\"Initializing the LDMPipeline with pruned models...\")\n",
    "pipeline = LDMPipeline(\n",
    "    unet=unet,\n",
    "    vqvae=vqvae,\n",
    "    scheduler=scheduler,\n",
    ").to(DEVICE)\n",
    "\n",
    "# ============================\n",
    "# 6. Define the Training Loop\n",
    "# ============================\n",
    "\n",
    "def kl_divergence_loss(noise_pred, noise):\n",
    "    \"\"\"\n",
    "    Computes the KL Divergence between the predicted noise distribution and the actual noise.\n",
    "\n",
    "    Parameters:\n",
    "    - noise_pred (Tensor): Predicted noise by the model.\n",
    "    - noise (Tensor): True Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: KL Divergence loss value.\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities (for demonstration, adjust if necessary)\n",
    "    noise_pred_log_probs = F.log_softmax(noise_pred, dim=-1)\n",
    "    noise_probs = F.softmax(noise, dim=-1)\n",
    "    \n",
    "    # Compute KL Divergence\n",
    "    kl_loss = F.kl_div(noise_pred_log_probs, noise_probs, reduction='batchmean')\n",
    "    return kl_loss\n",
    "\n",
    "def generate_sample_images(pipeline, num_images=1, prompt=\"A high quality portrait\"):\n",
    "    pipeline.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        generated_images = pipeline(batch_size=num_images, num_inference_steps=100).images\n",
    "    for img in generated_images:\n",
    "        display(img)\n",
    "\n",
    "# Move models to device\n",
    "unet.to(DEVICE)\n",
    "vqvae.to(DEVICE)\n",
    "\n",
    "# Set models to training mode\n",
    "unet.train()\n",
    "vqvae.train()\n",
    "\n",
    "# Define optimizer (only parameters that require gradients)\n",
    "optimizer = optim.Adam(\n",
    "    list(unet.parameters()) + list(vqvae.parameters()),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Define a loss function, e.g., Mean Squared Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        images = batch.to(DEVICE)\n",
    "        \n",
    "        # Forward pass through VQ-VAE to get latent representations\n",
    "        with torch.no_grad():\n",
    "            vqvae_output = vqvae.encode(images)\n",
    "            # Corrected line: Use 'latent_sample' instead of 'latent_dist.sample()'\n",
    "            latents = vqvae_output.latents\n",
    "            latents = latents * vqvae.config.scaling_factor\n",
    "        \n",
    "        # Add noise according to the scheduler\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (latents.shape[0],), device=DEVICE).long()\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # Forward pass through U-Net\n",
    "        noise_pred = unet(noisy_latents, timesteps).sample\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(noise_pred, noise)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Mask gradients of pruned weights\n",
    "        with torch.no_grad():\n",
    "            for module in unet.modules():\n",
    "                if isinstance(module, nn.Conv2d) and module.weight.grad is not None:\n",
    "                    zero_mask = module.weight == 0\n",
    "                    if zero_mask.any():\n",
    "                        module.weight.grad[zero_mask] = 0\n",
    "            for module in vqvae.modules():\n",
    "                if isinstance(module, nn.Conv2d) and module.weight.grad is not None:\n",
    "                    zero_mask = module.weight == 0\n",
    "                    if zero_mask.any():\n",
    "                        module.weight.grad[zero_mask] = 0\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Re-mask the weights to ensure pruned weights remain zero\n",
    "        with torch.no_grad():\n",
    "            for module in unet.modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    module.weight[module.weight == 0] = 0\n",
    "            for module in vqvae.modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    module.weight[module.weight == 0] = 0\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Save the model at each epoch\n",
    "    torch.save(unet.state_dict(), FINE_TUNED_UNET_PATH_TEMPLATE.format(epoch+1))\n",
    "    torch.save(vqvae.state_dict(), FINE_TUNED_VQVAE_PATH_TEMPLATE.format(epoch+1))\n",
    "    print(f\"Saved fine-tuned models for epoch {epoch+1}\")\n",
    "    \n",
    "    # Uncomment the following line to generate sample images after training\n",
    "    generate_sample_images(pipeline, num_images=1, prompt=\"A high quality portrait\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "result_folder = \"result_70\"\n",
    "def generate_sample_images(pipeline, num_images=10, num_inference_steps_list=[20, 100], num_samples=50000):\n",
    "    # Ensure the pipeline is on the correct device\n",
    "\n",
    "    # Iterate over each number of inference steps (10, 20, 100)\n",
    "    for num_inference_steps in num_inference_steps_list:\n",
    "        # Create a subfolder for each number of inference steps\n",
    "        steps_folder = os.path.join(result_folder, f\"{num_inference_steps}_steps\")\n",
    "        if not os.path.exists(steps_folder):\n",
    "            os.makedirs(steps_folder)\n",
    "\n",
    "        # Generate images in batches, aiming for the total number of images\n",
    "        num_batches = num_samples // num_images\n",
    "        for batch_idx in range(num_batches):\n",
    "            with torch.no_grad():\n",
    "                # Generate images using the pipeline\n",
    "                generated_images = pipeline(batch_size=num_images, num_inference_steps=num_inference_steps).images\n",
    "\n",
    "            # Save each generated image to the corresponding subfolder\n",
    "            for i, img in enumerate(generated_images):\n",
    "                # Create a filename for the image\n",
    "                img_path = os.path.join(steps_folder, f\"generated_image_{batch_idx * num_images + i + 1}.png\")\n",
    "                \n",
    "                # Save the image\n",
    "                img.save(img_path)\n",
    "                print(f\"Image {batch_idx * num_images + i + 1} saved at: {img_path}\")\n",
    "\n",
    "# Example usage:\n",
    "generate_sample_images(pipeline, num_images=10, num_inference_steps_list=[20, 100, 50], num_samples=50000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
