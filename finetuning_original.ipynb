{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 21:45:49.670342: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-05 21:45:49.702782: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-05 21:45:49.702819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-05 21:45:49.703675: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-05 21:45:49.709210: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 21:45:50.523671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "An error occurred while trying to fetch CompVis/ldm-celebahq-256: CompVis/ldm-celebahq-256 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "/home/cqilab/anaconda3/envs/sgrs/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "An error occurred while trying to fetch CompVis/ldm-celebahq-256: CompVis/ldm-celebahq-256 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "/home/cqilab/anaconda3/envs/sgrs/lib/python3.11/site-packages/diffusers/configuration_utils.py:245: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "The config attributes {'timestep_values': None, 'timesteps': 1000} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the LDMPipeline with pruned models...\n",
      "\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/15000 [00:00<?, ?it/s]/home/cqilab/anaconda3/envs/sgrs/lib/python3.11/site-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDIMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDIMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 178\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    177\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 178\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Mask gradients of pruned weights\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/sgrs/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sgrs/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from diffusers import LDMPipeline, UNet2DModel, VQModel, DDIMScheduler\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display\n",
    "\n",
    "# ============================\n",
    "# 1. Configuration Parameters\n",
    "# ============================\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"dataset/celeba_hq_256\"\n",
    "OUTPUT_DIR = \"generated_images\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 8\n",
    "NUM_WORKERS = 4\n",
    "IMAGE_SIZE = 256  # Assuming CelebA-HQ images are 256x256\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================\n",
    "# 2. Define the Custom Dataset\n",
    "# ============================\n",
    "\n",
    "class CelebaHQDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset directory.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = [\n",
    "            os.path.join(root_dir, img) for img in os.listdir(root_dir)\n",
    "            if img.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image in case of error\n",
    "            image = Image.new(\"RGB\", (IMAGE_SIZE, IMAGE_SIZE), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# ============================\n",
    "# 3. Prepare DataLoader\n",
    "# ============================\n",
    "\n",
    "# Define transformations (resize and normalize as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset = CelebaHQDataset(root_dir=DATASET_PATH, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ============================\n",
    "# 4. Load and Prune the Models\n",
    "# ============================\n",
    "\n",
    "# Load the pretrained models and scheduler\n",
    "unet = UNet2DModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"unet\")\n",
    "vqvae = VQModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"vqvae\")\n",
    "scheduler = DDIMScheduler.from_config(\"CompVis/ldm-celebahq-256\", subfolder=\"scheduler\")\n",
    "\n",
    "\n",
    "# Initialize the pipeline with the pruned U-Net and VQ-VAE\n",
    "print(\"Initializing the LDMPipeline with pruned models...\")\n",
    "pipeline = LDMPipeline(\n",
    "    unet=unet,\n",
    "    vqvae=vqvae,\n",
    "    scheduler=scheduler,\n",
    ").to(DEVICE)\n",
    "\n",
    "# ============================\n",
    "# 6. Define the Training Loop\n",
    "# ============================\n",
    "\n",
    "def kl_divergence_loss(noise_pred, noise):\n",
    "    \"\"\"\n",
    "    Computes the KL Divergence between the predicted noise distribution and the actual noise.\n",
    "\n",
    "    Parameters:\n",
    "    - noise_pred (Tensor): Predicted noise by the model.\n",
    "    - noise (Tensor): True Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: KL Divergence loss value.\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities (for demonstration, adjust if necessary)\n",
    "    noise_pred_log_probs = F.log_softmax(noise_pred, dim=-1)\n",
    "    noise_probs = F.softmax(noise, dim=-1)\n",
    "    \n",
    "    # Compute KL Divergence\n",
    "    kl_loss = F.kl_div(noise_pred_log_probs, noise_probs, reduction='batchmean')\n",
    "    return kl_loss\n",
    "\n",
    "def generate_sample_images(pipeline, num_images=1, prompt=\"A high quality portrait\"):\n",
    "    pipeline.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        generated_images = pipeline(batch_size=num_images, num_inference_steps=100).images\n",
    "    for img in generated_images:\n",
    "        display(img)\n",
    "\n",
    "# Move models to device\n",
    "unet.to(DEVICE)\n",
    "vqvae.to(DEVICE)\n",
    "\n",
    "# Set models to training mode\n",
    "unet.train()\n",
    "vqvae.train()\n",
    "\n",
    "# Define optimizer (only parameters that require gradients)\n",
    "optimizer = optim.Adam(\n",
    "    list(unet.parameters()) + list(vqvae.parameters()),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Define a loss function, e.g., Mean Squared Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        images = batch.to(DEVICE)\n",
    "        \n",
    "        # Forward pass through VQ-VAE to get latent representations\n",
    "        with torch.no_grad():\n",
    "            vqvae_output = vqvae.encode(images)\n",
    "            # Corrected line: Use 'latent_sample' instead of 'latent_dist.sample()'\n",
    "            latents = vqvae_output.latents\n",
    "            latents = latents * vqvae.config.scaling_factor\n",
    "        \n",
    "        # Add noise according to the scheduler\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (latents.shape[0],), device=DEVICE).long()\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # Forward pass through U-Net\n",
    "        noise_pred = unet(noisy_latents, timesteps).sample\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(noise_pred, noise)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Mask gradients of pruned weights\n",
    "        with torch.no_grad():\n",
    "            for module in unet.modules():\n",
    "                if isinstance(module, nn.Conv2d) and module.weight.grad is not None:\n",
    "                    zero_mask = module.weight == 0\n",
    "                    if zero_mask.any():\n",
    "                        module.weight.grad[zero_mask] = 0\n",
    "            for module in vqvae.modules():\n",
    "                if isinstance(module, nn.Conv2d) and module.weight.grad is not None:\n",
    "                    zero_mask = module.weight == 0\n",
    "                    if zero_mask.any():\n",
    "                        module.weight.grad[zero_mask] = 0\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Re-mask the weights to ensure pruned weights remain zero\n",
    "        with torch.no_grad():\n",
    "            for module in unet.modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    module.weight[module.weight == 0] = 0\n",
    "            for module in vqvae.modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    module.weight[module.weight == 0] = 0\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Save the model at each epoch\n",
    "    print(f\"Saved fine-tuned models for epoch {epoch+1}\")\n",
    "    \n",
    "    # Uncomment the following line to generate sample images after training\n",
    "    generate_sample_images(pipeline, num_images=1, prompt=\"A high quality portrait\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "result_folder = \"result_original_finetuned\"\n",
    "def generate_sample_images(pipeline, num_images=10, num_inference_steps_list=[10, 20, 100], num_samples=50000):\n",
    "    # Ensure the pipeline is on the correct device\n",
    "\n",
    "    # Iterate over each number of inference steps (10, 20, 100)\n",
    "    for num_inference_steps in num_inference_steps_list:\n",
    "        # Create a subfolder for each number of inference steps\n",
    "        steps_folder = os.path.join(result_folder, f\"{num_inference_steps}_steps\")\n",
    "        if not os.path.exists(steps_folder):\n",
    "            os.makedirs(steps_folder)\n",
    "\n",
    "        # Generate images in batches, aiming for the total number of images\n",
    "        num_batches = num_samples // num_images\n",
    "        for batch_idx in range(num_batches):\n",
    "            with torch.no_grad():\n",
    "                # Generate images using the pipeline\n",
    "                generated_images = pipeline(batch_size=num_images, num_inference_steps=num_inference_steps).images\n",
    "\n",
    "            # Save each generated image to the corresponding subfolder\n",
    "            for i, img in enumerate(generated_images):\n",
    "                # Create a filename for the image\n",
    "                img_path = os.path.join(steps_folder, f\"generated_image_{batch_idx * num_images + i + 1}.png\")\n",
    "                \n",
    "                # Save the image\n",
    "                img.save(img_path)\n",
    "                print(f\"Image {batch_idx * num_images + i + 1} saved at: {img_path}\")\n",
    "\n",
    "# Example usage:\n",
    "generate_sample_images(pipeline, num_images=10, num_inference_steps_list=[10, 20, 100, 50], num_samples=50000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
